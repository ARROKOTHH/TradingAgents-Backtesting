# 5.2 LLM Adapters

A key design principle of this project is flexibility in choosing the Large Language Model (LLM). This is achieved through the **Adapter Pattern**, which is implemented in the `tradingagents/llm_adapters/` directory.

## The Adapter Pattern

The Adapter Pattern allows us to create a consistent, unified interface for multiple, different LLM providers. This means the rest of the application doesn't need to know the specific details of how to call the OpenAI API versus the Google Gemini API. It simply interacts with our universal `BaseLLM` adapter.

**Benefits of this approach:**
-   **Pluggability**: It's easy to add support for a new LLM provider without changing the core application logic. You just need to create a new adapter.
-   **Maintainability**: Code that uses an LLM is cleaner and not cluttered with provider-specific logic.
-   **Centralized Control**: Features like token counting, cost calculation, and standardized error handling are implemented once in the base adapter and inherited by all specific adapters.

## Configuration and Usage Flow

The system ensures that the user's choice of LLM in the Web UI is respected throughout the application.

1.  **UI Selection**: In the `web/components/sidebar.py`, the user selects an LLM Provider (e.g., "Google") and the models for "Deep Think" and "Fast Think" tasks. This choice is stored in Streamlit's `session_state`.

2.  **Configuration Object**: The main web application file, `web/app.py`, reads this state and creates a `config` dictionary.

3.  **Adapter Factory**: When an agent or module needs an LLM, it uses a factory function (e.g., `create_google_openai_llm`) from the `llm_adapters` directory. This function takes the `config` object and returns the appropriate, fully configured LLM adapter instance.

4.  **Dependency Injection**: This LLM instance is then passed ("injected") into the agent that needs to use it.

This robust flow ensures that the correct LLM is always used for the right task, as specified by the user.

## Supported Providers

The project currently has built-in support for:
-   **OpenAI** (GPT series)
-   **Google** (Gemini series)
-   **DeepSeek**
-   **Alibaba Qwen** (via Dashscope)

## How to Add a New Adapter

To add support for a new LLM provider, you would typically:
1.  Create a new file in `tradingagents/llm_adapters/`, e.g., `my_new_llm_adapter.py`.
2.  Create a new class that inherits from `OpenAICompatibleBase` or another suitable base class.
3.  Implement the provider-specific logic for API calls.
4.  Add a new factory function to create an instance of your new adapter.
5.  Update the UI in `web/components/sidebar.py` and the user configuration in `config/models.json` to make the new provider an option.
